#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivè®ºæ–‡çˆ¬å–å™¨ - ç‹¬ç«‹çš„è®ºæ–‡æ•°æ®çˆ¬å–å·¥å…·
"""

import os
import sys
import subprocess
import json
from datetime import datetime
from pathlib import Path

from config_loader import get_config
from logger_setup import setup_logger, get_logger

# æ·»åŠ ä¸»é¡¹ç›®ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥å…±äº«æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# =============================================================================
# é…ç½®åŒºåŸŸ
# =============================================================================


class CrawlConfig:
    """çˆ¬å–é…ç½®ç±»"""

    def __init__(self, date=None, config_path=None):
        """
        åˆå§‹åŒ–é…ç½®

        Args:
            date: æŒ‡å®šçš„æ—¥æœŸ
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.date = date or datetime.now().strftime("%Y-%m-%d")

        # ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
        self.yaml_config = get_config(config_path)

        # ä»é…ç½®æ–‡ä»¶è¯»å–è®¾ç½®
        self.CATEGORIES = self.yaml_config.categories
        self.BASE_DATA_DIR = self.yaml_config.base_data_dir
        self.DOWNLOAD_PDF = self.yaml_config.download_pdf
        self.PDF_TIMEOUT = self.yaml_config.pdf_timeout
        self.PDF_MAX_SIZE_MB = self.yaml_config.pdf_max_size_mb
        self.CONCURRENT_DOWNLOADS = self.yaml_config.concurrent_downloads


# =============================================================================
# ä¸»ç¨‹åºç±»
# =============================================================================

class ArxivCrawler:
    """arXivè®ºæ–‡çˆ¬å–å™¨"""

    def __init__(self, config: CrawlConfig):
        self.config = config
        self.today = config.date
        self.logger = get_logger()
        # è®°å½•çˆ¬å–æ—¥æœŸçš„æ–‡ä»¶è·¯å¾„
        self.last_crawl_file = Path(self.config.BASE_DATA_DIR) / ".last_crawl_date"
        self.setup_environment()

    def _get_last_crawl_date(self) -> str:
        """è·å–ä¸Šæ¬¡çˆ¬å–çš„æ—¥æœŸ"""
        if self.last_crawl_file.exists():
            try:
                return self.last_crawl_file.read_text().strip()
            except Exception:
                return ""
        return ""

    def _save_crawl_date(self):
        """ä¿å­˜å½“å‰çˆ¬å–æ—¥æœŸ"""
        try:
            self.last_crawl_file.parent.mkdir(parents=True, exist_ok=True)
            self.last_crawl_file.write_text(self.today)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä¿å­˜çˆ¬å–æ—¥æœŸ: {e}")

    def check_should_crawl(self, force: bool = False) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶çˆ¬å–
            
        Returns:
            æ˜¯å¦åº”è¯¥ç»§ç»­çˆ¬å–
        """
        if force:
            print("ğŸ”„ å¼ºåˆ¶çˆ¬å–æ¨¡å¼ï¼Œè·³è¿‡æ—¥æœŸæ£€æŸ¥")
            return True
        
        last_date = self._get_last_crawl_date()
        if last_date == self.today:
            print(f"â­ï¸ ä»Šæ—¥({self.today})å·²çˆ¬å–è¿‡ï¼Œè·³è¿‡é‡å¤çˆ¬å–")
            print(f"   å¦‚éœ€å¼ºåˆ¶çˆ¬å–ï¼Œè¯·ä½¿ç”¨ --force å‚æ•°")
            return False
        
        if last_date:
            print(f"ğŸ“… ä¸Šæ¬¡çˆ¬å–æ—¥æœŸ: {last_date}ï¼Œä»Šæ—¥: {self.today}ï¼Œå¼€å§‹æ–°çš„çˆ¬å–")
        else:
            print(f"ğŸ“… é¦–æ¬¡çˆ¬å–ï¼Œæ—¥æœŸ: {self.today}")
        
        return True

    def _get_category_short(self, category):
        """æå–åˆ†ç±»çŸ­åç§°: cs.IR -> IR"""
        return category.split('.')[-1]

    def _get_category_paths(self, category_short):
        """è·å–åˆ†ç±»çš„æ•°æ®è·¯å¾„ï¼ˆä¸å†æŒ‰æ—¥æœŸåŒºåˆ†ï¼‰"""
        base = self.config.BASE_DATA_DIR
        return {
            'jsonl': f"{base}/jsonl/{category_short}",
        }

    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡å’Œåˆ›å»ºç›®å½•"""
        os.environ["CATEGORIES"] = ",".join(self.config.CATEGORIES)
        os.environ["TARGET_DATE"] = self.today
        os.environ["BASE_DATA_DIR"] = self.config.BASE_DATA_DIR

        # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ›å»ºç›®å½•
        for category in self.config.CATEGORIES:
            category_short = self._get_category_short(category)
            paths = self._get_category_paths(category_short)
            Path(paths['jsonl']).mkdir(parents=True, exist_ok=True)

    def print_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        print("=" * 60)
        print("ğŸ•·ï¸  arXivè®ºæ–‡çˆ¬å–å™¨")
        print("=" * 60)
        print("ğŸ“‹ å½“å‰é…ç½®:")
        print(f"   åˆ†ç±»: {', '.join(self.config.CATEGORIES)}")
        print(f"   æ—¥æœŸ: {self.today}")
        print(f"   æ•°æ®ç›®å½•: {self.config.BASE_DATA_DIR}")
        print(f"   ä¸‹è½½PDF: {self.config.DOWNLOAD_PDF}")
        print()

    def _get_data_file(self, category_short):
        """è·å–åˆ†ç±»çš„æ•°æ®æ–‡ä»¶è·¯å¾„"""
        paths = self._get_category_paths(category_short)
        return f"{paths['jsonl']}/papers.jsonl"

    def crawl_papers(self):
        """çˆ¬å–è®ºæ–‡æ•°æ®"""
        print("ğŸš€ å¼€å§‹çˆ¬å–arXivè®ºæ–‡...")

        # æ¸…ç©ºæ‰€æœ‰åˆ†ç±»çš„æ•°æ®ç›®å½•
        for category in self.config.CATEGORIES:
            category_short = self._get_category_short(category)
            paths = self._get_category_paths(category_short)
            jsonl_dir = Path(paths['jsonl'])
            if jsonl_dir.exists():
                # æ¸…ç©ºç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
                for file in jsonl_dir.glob("*"):
                    if file.is_file():
                        print(f"ğŸ—‘ï¸ æ¸…ç†æ—§æ•°æ®: {file}")
                        file.unlink()

        # æ‰§è¡Œçˆ¬å–
        try:
            cmd = ["scrapy", "crawl", "arxiv"]
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

            subprocess.run(
                cmd, cwd="crawler", capture_output=True, text=True, check=True
            )
            print("âœ… çˆ¬å–å®Œæˆ")
            return True

        except subprocess.CalledProcessError as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}\né”™è¯¯è¾“å‡º: {e.stderr}")
            return False
        except FileNotFoundError:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ°scrapyå‘½ä»¤\nè¯·ç¡®ä¿å·²å®‰è£…scrapy: pip install scrapy")
            return False

    def check_duplicates(self):
        """æ£€æŸ¥å»é‡"""
        print("ğŸ” æ‰§è¡Œå»é‡æ£€æŸ¥...")
        try:
            result = subprocess.run(
                ["python", "crawler/check_stats.py"],
                capture_output=True, text=True
            )

            if result.returncode in [0, 1]:
                msg = "æœ‰æ–°å†…å®¹" if result.returncode == 0 else "æ— æ–°å†…å®¹ï¼Œä½†æ•°æ®å·²ä¿å­˜"
                print(f"âœ… å»é‡æ£€æŸ¥å®Œæˆï¼Œ{msg}")
                return True
            else:
                print(f"âŒ å»é‡æ£€æŸ¥é”™è¯¯: {result.stderr}")
                return False
        except Exception as e:
            print(f"âŒ å»é‡æ£€æŸ¥å¼‚å¸¸: {e}")
            return False

    def _print_category_stats(self, category):
        """æ‰“å°å•ä¸ªåˆ†ç±»çš„ç»Ÿè®¡ä¿¡æ¯"""
        category_short = self._get_category_short(category)
        paths = self._get_category_paths(category_short)
        data_file = self._get_data_file(category_short)

        print(f"\nğŸ“ {category} åˆ†ç±»ç»“æœ:")
        print(f"   ğŸ“‚ æ•°æ®ç›®å½•: {paths['jsonl']}/")

        # ç»Ÿè®¡è®ºæ–‡æ•°
        if os.path.exists(data_file):
            with open(data_file, 'r') as f:
                count = sum(1 for _ in f)
            print(f"   âœ… è®ºæ–‡æ•°æ®: {os.path.basename(data_file)} ({count} ç¯‡)")

    def _print_sample_paper(self, category):
        """æ‰“å°å•ä¸ªåˆ†ç±»çš„ç¤ºä¾‹è®ºæ–‡"""
        category_short = self._get_category_short(category)
        data_file = self._get_data_file(category_short)

        if not os.path.exists(data_file):
            return

        print(f"\nğŸ“ {category} ç¤ºä¾‹è®ºæ–‡:")
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                sample = json.loads(f.readline())
                print(f"   ID: {sample.get('id', 'N/A')}")
                print(f"   æ ‡é¢˜: {sample.get('title', 'N/A')[:80]}...")
                print(f"   ä½œè€…: {', '.join(sample.get('authors', [])[:3])}...")
        except Exception as e:
            print(f"   æ— æ³•æ˜¾ç¤ºç¤ºä¾‹: {e}")

    def show_results(self):
        """æ˜¾ç¤ºçˆ¬å–ç»“æœ"""
        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬å–å®Œæˆ")
        print("=" * 60)

        # æ˜¾ç¤ºæ¯ä¸ªåˆ†ç±»çš„ç»Ÿè®¡ä¿¡æ¯
        for category in self.config.CATEGORIES:
            self._print_category_stats(category)

        # æ˜¾ç¤ºç¤ºä¾‹è®ºæ–‡
        for category in self.config.CATEGORIES:
            self._print_sample_paper(category)

    def run(self, force: bool = False):
        """
        è¿è¡Œå®Œæ•´æµç¨‹
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶çˆ¬å–ï¼ˆå¿½ç•¥æ—¥æœŸæ£€æŸ¥ï¼‰
        """
        self.print_config()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
        if not self.check_should_crawl(force=force):
            return True  # è·³è¿‡ä½†ä¸ç®—å¤±è´¥
        
        success = (
            self.crawl_papers()
            and self.check_duplicates()
        )
        
        if success:
            # ä¿å­˜çˆ¬å–æ—¥æœŸ
            self._save_crawl_date()
            self.show_results()
        
        return success


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="arXivè®ºæ–‡çˆ¬å–å™¨")
    parser.add_argument("--date", help="æŒ‡å®šå¤„ç†æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„", default="./config.yaml")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶çˆ¬å–ï¼Œå¿½ç•¥æ—¥æœŸæ£€æŸ¥")
    args = parser.parse_args()

    print("ğŸš€ å¯åŠ¨arXivè®ºæ–‡çˆ¬å–å™¨...")

    # åŠ è½½é…ç½®
    try:
        yaml_config = get_config(args.config)
        yaml_config.ensure_directories()
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return False

    # è®¾ç½®æ—¥å¿—
    log_file = yaml_config.get_log_file('daily_arxiv')
    logger = setup_logger(
        name="daily_arxiv",
        log_file=log_file,
        level=yaml_config.log_level,
        enable_rotation=yaml_config.enable_log_rotation,
        max_bytes=yaml_config.log_max_bytes,
        backup_count=yaml_config.log_backup_count
    )
    logger.info("å¯åŠ¨arXivè®ºæ–‡çˆ¬å–å™¨")

    # æ£€æŸ¥ä¾èµ–
    if not os.path.exists("crawler"):
        print("âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦ç›®å½• crawler")
        logger.error("ç¼ºå°‘å¿…è¦ç›®å½• crawler")
        return False

    try:
        config = CrawlConfig(date=args.date, config_path=args.config)
        crawler = ArxivCrawler(config)
        success = crawler.run(force=args.force)

        if success:
            print("\nğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
            logger.info("çˆ¬å–ä»»åŠ¡å®Œæˆ")
        else:
            print("\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
            logger.error("çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
        return success

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
        logger.warning("ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
        return False
    except Exception as e:
        print(f"\nâŒ æœªé¢„æœŸçš„é”™è¯¯: {e}")
        logger.exception(f"æœªé¢„æœŸçš„é”™è¯¯: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
