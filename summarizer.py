"""
LLMè®ºæ–‡æ€»ç»“æ¨¡å— - ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡è®ºæ–‡æŠ¥å‘Š
"""
import os
from typing import List, Dict, Tuple, Optional, Union
from openai import OpenAI

from matcher import Paper


class PaperSummarizer:
    """è®ºæ–‡æ€»ç»“å™¨ - ç”Ÿæˆé«˜è´¨é‡çš„è®ºæ–‡è§£è¯»æŠ¥å‘Š"""
    
    def __init__(
        self,
        model: str = "deepseek-chat",
        api_key: str = "",
        base_url: str = "https://api.deepseek.com"
    ):
        """
        åˆå§‹åŒ–æ€»ç»“å™¨
        
        Args:
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
        """
        self.model = model
        self.api_key = api_key or os.environ.get('LLM_API_KEY', '')
        self.base_url = base_url
        
        if not self.api_key:
            raise ValueError("æœªæä¾›LLM APIå¯†é’¥ï¼Œè¯·è®¾ç½®LLM_API_KEYç¯å¢ƒå˜é‡æˆ–åœ¨é…ç½®ä¸­æŒ‡å®š")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
    
    def _call_llm(self, system_prompt: str, user_prompt: str, max_tokens: int = 2000) -> str:
        """è°ƒç”¨LLM"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def summarize_paper_batch(
        self, 
        papers: List[Paper], 
        keywords: Union[List[str], Dict[str, float]]
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆå¤šç¯‡è®ºæ–‡çš„è¯¦ç»†è§£è¯»ï¼ˆä¸€æ¬¡LLMè°ƒç”¨ï¼‰
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆå»ºè®®ä¸è¶…è¿‡5ç¯‡ï¼‰
            keywords: ç”¨æˆ·å…³æ³¨çš„å…³é”®è¯
        
        Returns:
            æ¯ç¯‡è®ºæ–‡çš„è§£è¯»åˆ—è¡¨
        """
        # å¤„ç†å…³é”®è¯æ ¼å¼
        if isinstance(keywords, dict):
            kw_list = list(keywords.keys())
        else:
            kw_list = keywords
        
        # æ„å»ºå¤šç¯‡è®ºæ–‡çš„è¾“å…¥
        papers_text = ""
        for i, paper in enumerate(papers, 1):
            papers_text += f"""
---
## è®ºæ–‡ {i}
**æ ‡é¢˜**: {paper.title}
**æ‘˜è¦**: {paper.summary}
---
"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AI/MLç ”ç©¶å‘˜ï¼Œæ“…é•¿ç”¨é€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡è§£è¯»å­¦æœ¯è®ºæ–‡ã€‚
ä½ çš„è§£è¯»åº”è¯¥ç®€æ´ã€ä¸“ä¸šã€æœ‰æ·±åº¦ã€‚"""

        user_prompt = f"""è¯·å¯¹ä»¥ä¸‹ {len(papers)} ç¯‡arXivè®ºæ–‡åˆ†åˆ«è¿›è¡Œç®€è¦è§£è¯»ã€‚

**ç”¨æˆ·å…³æ³¨é¢†åŸŸ**: {', '.join(kw_list)}

{papers_text}

è¯·å¯¹æ¯ç¯‡è®ºæ–‡ä»ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢è¿›è¡Œè§£è¯»ï¼š

1. **ç ”ç©¶èƒŒæ™¯ä¸æŒ‘æˆ˜**ï¼ˆ300-350å­—ï¼‰ï¼šè¿™ä¸ªé¢†åŸŸçš„ç°çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿè®ºæ–‡è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

2. **æ–¹æ³•ä¸å®éªŒç»“è®º**ï¼ˆ300-350å­—ï¼‰ï¼šè®ºæ–‡æå‡ºäº†ä»€ä¹ˆæ–¹æ³•ï¼Ÿå–å¾—äº†ä»€ä¹ˆæ•ˆæœï¼Ÿ

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- æ¯ç¯‡è®ºæ–‡ä¹‹é—´ç”¨ "===è®ºæ–‡N===" åˆ†éš”ï¼ˆNä¸ºè®ºæ–‡åºå·1,2,3...ï¼‰
- æ¯ç¯‡è®ºæ–‡åŒ…å«ä¸¤ä¸ªæ®µè½ï¼Œæ®µè½ä¹‹é—´ç©ºä¸€è¡Œ
- ä¸éœ€è¦åŠ ç²—æˆ–æ ‡é¢˜ï¼Œç›´æ¥å†™å†…å®¹

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
===è®ºæ–‡1===
è¿™ä¸ªé¢†åŸŸç›®å‰é¢ä¸´...ç ”ç©¶èƒŒæ™¯ä¸æŒ‘æˆ˜çš„æè¿°...

è¯¥è®ºæ–‡æå‡ºäº†...æ–¹æ³•ä¸å®éªŒç»“è®ºçš„æè¿°...
===è®ºæ–‡2===
...
"""

        # æ ¹æ®è®ºæ–‡æ•°é‡è°ƒæ•´ max_tokens
        max_tokens = min(1200 * len(papers), 4000)
        result = self._call_llm(system_prompt, user_prompt, max_tokens=max_tokens)
        
        # è§£æç»“æœï¼ŒæŒ‰è®ºæ–‡åˆ†å‰²
        summaries = []
        parts = result.split("===è®ºæ–‡")
        
        for part in parts[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºéƒ¨åˆ†
            # ç§»é™¤è®ºæ–‡åºå·æ ‡è®°
            content = part.strip()
            if content and content[0].isdigit():
                # ç§»é™¤ "1===" è¿™æ ·çš„å‰ç¼€
                idx = content.find("===")
                if idx != -1:
                    content = content[idx + 3:].strip()
                elif len(content) > 1:
                    content = content[1:].strip()  # åªç§»é™¤æ•°å­—
            summaries.append(content)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç®€å•åˆ†å‰²
        if len(summaries) != len(papers):
            # å›é€€ï¼šå‡åˆ†ç»“æœ
            lines = result.split('\n')
            chunk_size = max(len(lines) // len(papers), 1)
            summaries = []
            for i in range(len(papers)):
                start = i * chunk_size
                end = start + chunk_size if i < len(papers) - 1 else len(lines)
                summaries.append('\n'.join(lines[start:end]))
        
        return summaries
    
    def _generate_paper_table(self, summaries: List[Dict]) -> str:
        """
        ç”Ÿæˆè®ºæ–‡æ±‡æ€»è¡¨æ ¼
        
        Args:
            summaries: è®ºæ–‡æ€»ç»“åˆ—è¡¨
        
        Returns:
            Markdownæ ¼å¼çš„è¡¨æ ¼
        """
        if not summaries:
            return ""
        
        # è¡¨å¤´
        table = "| # | æ ‡é¢˜ | ä½œè€… | åŒ¹é…å…³é”®è¯ | å¾—åˆ† |\n"
        table += "|:---:|:---|:---|:---|:---:|\n"
        
        # è¡¨æ ¼å†…å®¹
        for i, item in enumerate(summaries, 1):
            paper = item['paper']
            details = item['match_details']
            
            # å®Œæ•´æ ‡é¢˜
            title = paper.title
            
            # å®Œæ•´ä½œè€…åˆ—è¡¨
            authors_str = ', '.join(paper.authors)
            
            # åŒ¹é…å…³é”®è¯
            matched = details.get('all_matched', [])
            matched_str = ', '.join(matched) if matched else "-"
            
            # å¾—åˆ†
            score = f"{paper.relevance_score:.2f}"
            
            table += f"| {i} | {title} | {authors_str} | {matched_str} | {score} |\n"
        
        return table
    
    def generate_daily_overview(
        self, 
        papers: List[Tuple[Paper, Dict]], 
        keywords: Union[List[str], Dict[str, float]]
    ) -> str:
        """
        ç”Ÿæˆä»Šæ—¥è®ºæ–‡çš„æ•´ä½“æ€»ç»“
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            keywords: å…³é”®è¯
        
        Returns:
            ä»Šæ—¥ç ”ç©¶æ–¹å‘æ€»ç»“
        """
        # å¤„ç†å…³é”®è¯æ ¼å¼
        if isinstance(keywords, dict):
            kw_list = list(keywords.keys())
        else:
            kw_list = keywords
        
        # æ„å»ºè®ºæ–‡æ‘˜è¦åˆ—è¡¨
        paper_briefs = []
        for i, (paper, details) in enumerate(papers[:10], 1):  # æœ€å¤šå–å‰10ç¯‡
            matched = details.get('all_matched', [])
            paper_briefs.append(f"{i}. ã€Š{paper.title}ã€‹\n   åŒ¹é…å…³é”®è¯: {', '.join(matched)}")
        
        papers_text = '\n'.join(paper_briefs)
        
        system_prompt = """ä½ æ˜¯ä¸€ä½å­¦æœ¯é¢†åŸŸçš„èµ„æ·±åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¤šç¯‡è®ºæ–‡ä¸­æç‚¼ç ”ç©¶è¶‹åŠ¿å’Œæ–¹å‘ã€‚
ä½ çš„æ€»ç»“åº”è¯¥ï¼š
- é«˜å±‹å»ºç“´ï¼ŒæŠŠæ¡æ•´ä½“æ–¹å‘
- æŒ‡å‡ºä»Šæ—¥è®ºæ–‡çš„å…±åŒä¸»é¢˜å’Œçƒ­ç‚¹
- å¯¹è¯»è€…çš„ç ”ç©¶å·¥ä½œæœ‰å¯å‘æ„ä¹‰"""

        user_prompt = f"""ä»Šæ—¥ç­›é€‰å‡ºäº† {len(papers)} ç¯‡ä¸ç”¨æˆ·ç ”ç©¶æ–¹å‘ç›¸å…³çš„è®ºæ–‡ã€‚

**ç”¨æˆ·å…³æ³¨é¢†åŸŸ**: {', '.join(kw_list)}

**ä»Šæ—¥è®ºæ–‡åˆ—è¡¨**:
{papers_text}

è¯·ç”¨200-300å­—æ€»ç»“ä»Šæ—¥è®ºæ–‡çš„æ•´ä½“æƒ…å†µï¼š
1. ä»Šæ—¥è®ºæ–‡ä¸»è¦èšç„¦åœ¨å“ªäº›ç ”ç©¶æ–¹å‘ï¼Ÿ
2. æœ‰ä»€ä¹ˆå€¼å¾—å…³æ³¨çš„ç ”ç©¶è¶‹åŠ¿æˆ–çƒ­ç‚¹ï¼Ÿ
3. å¯¹ä»äº‹ç›¸å…³ç ”ç©¶çš„è¯»è€…æœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ

è¯·ç›´æ¥è¾“å‡ºæ€»ç»“å†…å®¹ï¼Œè¯­æ°”ä¸“ä¸šä½†ä¸å¤±äº²å’Œã€‚"""

        return self._call_llm(system_prompt, user_prompt, max_tokens=800)
    
    
    def summarize_papers(
        self,
        papers: List[Tuple[Paper, Dict]],
        keywords: Union[List[str], Dict[str, float]],
        batch_size: int = 3
    ) -> List[Dict]:
        """
        æ‰¹é‡æ€»ç»“è®ºæ–‡ï¼ˆä½¿ç”¨æ‰¹é‡LLMè°ƒç”¨æé«˜æ•ˆç‡ï¼‰
        
        Args:
            papers: [(è®ºæ–‡, åŒ¹é…è¯¦æƒ…), ...] åˆ—è¡¨
            keywords: å…³é”®è¯
            batch_size: æ¯æ¬¡LLMè°ƒç”¨å¤„ç†çš„è®ºæ–‡æ•°é‡
        
        Returns:
            [{'paper': Paper, 'summary': str, 'match_details': dict}, ...]
        """
        results = []
        total = len(papers)
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for batch_start in range(0, total, batch_size):
            batch_end = min(batch_start + batch_size, total)
            batch_papers = papers[batch_start:batch_end]
            
            # æ˜¾ç¤ºè¿›åº¦
            paper_nums = f"{batch_start+1}-{batch_end}"
            titles = [p[0].title[:30] + "..." for p in batch_papers]
            print(f"ğŸ“ æ­£åœ¨è§£è¯»è®ºæ–‡ [{paper_nums}/{total}]:")
            for t in titles:
                print(f"   - {t}")
            
            # æ‰¹é‡è°ƒç”¨LLM
            batch_paper_objs = [p[0] for p in batch_papers]
            summaries = self.summarize_paper_batch(batch_paper_objs, keywords)
            
            # ç»„è£…ç»“æœ
            for i, ((paper, details), summary) in enumerate(zip(batch_papers, summaries)):
                results.append({
                    'paper': paper,
                    'summary': summary,
                    'match_details': details
                })
        
        return results
    
    def generate_digest(
        self,
        summaries: List[Dict],
        keywords: Union[List[str], Dict[str, float]],
        date: str
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„è®ºæ–‡æŠ¥å‘Š
        
        æŠ¥å‘Šç»“æ„ï¼š
        1. æ ‡é¢˜
        2. ä»Šæ—¥è®ºæ–‡æ•´ä½“æ€»ç»“
        3. å„è®ºæ–‡è¯¦ç»†è§£è¯»ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
        
        Args:
            summaries: è®ºæ–‡æ€»ç»“åˆ—è¡¨
            keywords: å…³é”®è¯
            date: æ—¥æœŸ
        
        Returns:
            å®Œæ•´çš„æŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        # å¤„ç†å…³é”®è¯æ˜¾ç¤º
        if isinstance(keywords, dict):
            kw_display = ', '.join(f"{k}({v})" for k, v in keywords.items())
            kw_list = list(keywords.keys())
        else:
            kw_display = ', '.join(keywords)
            kw_list = keywords
        
        if not summaries:
            return f"""# arXiv è®ºæ–‡æ—¥æŠ¥ - {date}

ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨å…³æ³¨é¢†åŸŸç›¸å…³çš„è®ºæ–‡ã€‚
"""
        
        # é‡å»º papers åˆ—è¡¨ç”¨äºç”Ÿæˆæ¦‚è§ˆ
        papers = [(item['paper'], item['match_details']) for item in summaries]
        
        # 1. ç”Ÿæˆä»Šæ—¥æ¦‚è§ˆ
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆä»Šæ—¥è®ºæ–‡æ¦‚è§ˆ...")
        daily_overview = self.generate_daily_overview(papers, keywords)
        
        # 2. ç”Ÿæˆè®ºæ–‡æ±‡æ€»è¡¨æ ¼
        paper_table = self._generate_paper_table(summaries)
        
        # 3. æ„å»ºå®Œæ•´æŠ¥å‘Š
        report = f"""# arXiv è®ºæ–‡æ—¥æŠ¥

**æ—¥æœŸ**: {date}

**å…³æ³¨é¢†åŸŸ**: {kw_display}

**ä»Šæ—¥æ¨è**: {len(summaries)} ç¯‡ç›¸å…³è®ºæ–‡

---

## ä»Šæ—¥æ¦‚è§ˆ

{daily_overview}

### è®ºæ–‡ä¸€è§ˆè¡¨

{paper_table}

---

## è®ºæ–‡è¯¦è§£

"""
        # æ·»åŠ æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†è§£è¯»
        for i, item in enumerate(summaries, 1):
            paper = item['paper']
            summary = item['summary']
            
            # å…¨éƒ¨ä½œè€…
            authors_str = ', '.join(paper.authors)
            
            report += f"""### {i}. {paper.title} [[æŸ¥çœ‹è®ºæ–‡]]({paper.abs_url})

**ä½œè€…**: {authors_str}

{summary}

"""
        
        return report


def summarize_relevant_papers(
    papers: List[Tuple[Paper, Dict]],
    keywords: Union[List[str], Dict[str, float]],
    date: str,
    model: str = "deepseek-chat",
    api_key: str = "",
    base_url: str = "https://api.deepseek.com",
    batch_size: int = 3
) -> str:
    """
    æ€»ç»“ç›¸å…³è®ºæ–‡å¹¶ç”ŸæˆæŠ¥å‘Š
    
    Args:
        papers: ç›¸å…³è®ºæ–‡åˆ—è¡¨
        keywords: å…³é”®è¯ï¼ˆåˆ—è¡¨æˆ–å¸¦æƒé‡å­—å…¸ï¼‰
        date: æ—¥æœŸ
        model: LLMæ¨¡å‹
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        batch_size: æ¯æ¬¡LLMè°ƒç”¨å¤„ç†çš„è®ºæ–‡æ•°é‡
    
    Returns:
        å®Œæ•´çš„è®ºæ–‡æŠ¥å‘Š
    """
    summarizer = PaperSummarizer(model=model, api_key=api_key, base_url=base_url)
    summaries = summarizer.summarize_papers(papers, keywords, batch_size=batch_size)
    return summarizer.generate_digest(summaries, keywords, date)
